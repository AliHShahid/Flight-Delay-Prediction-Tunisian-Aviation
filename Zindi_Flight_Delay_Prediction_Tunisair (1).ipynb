{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "R7sNCwzFVibs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewamU1jZU02v"
      },
      "outputs": [],
      "source": [
        "# Schema-agnostic baseline with safe preprocessing + LightGBM\n",
        "import os, gc, warnings, re\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "from lightgbm import LGBMRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "VWuxW47HVpmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"/Train.csv\"\n",
        "TEST_PATH  = \"/Test.csv\"\n",
        "SAMPLE_SUB_PATH = \"/SampleSubmission.csv\"\n",
        "TARGET_COL = \"target\"\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS = 5"
      ],
      "metadata": {
        "id": "cTd2exqXVpEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "mhVV_G3HXkRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_id_col(df):\n",
        "    # Prefer “id”, “ID”, or any single column that looks like an identifier\n",
        "    candidates = [c for c in df.columns if c.lower() == \"id\"]\n",
        "    if candidates:\n",
        "        return candidates[0]\n",
        "    # fallback: look for something ending with id\n",
        "    for c in df.columns:\n",
        "        if re.search(r'\\bid\\b$', c, flags=re.IGNORECASE):\n",
        "            return c\n",
        "    # as a last resort, use the first column\n",
        "    return df.columns[0]\n",
        "\n",
        "def try_parse_datetime(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Try parsing to datetime; return parsed if success rate > 70%, else original.\"\"\"\n",
        "    s = pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
        "    ok = s.notna().mean()\n",
        "    return s if ok >= 0.7 else series\n",
        "\n",
        "def add_datetime_features(df, col):\n",
        "    # all UTC; derive basic features (no leakage)\n",
        "    s = df[col]\n",
        "    df[f\"{col}_year\"]   = s.dt.year\n",
        "    df[f\"{col}_month\"]  = s.dt.month\n",
        "    df[f\"{col}_day\"]    = s.dt.day\n",
        "    df[f\"{col}_dow\"]    = s.dt.dayofweek\n",
        "    df[f\"{col}_hour\"]   = s.dt.hour\n",
        "    df[f\"{col}_is_weekend\"] = (s.dt.dayofweek >= 5).astype(\"int8\")\n",
        "    # cyclic encodings\n",
        "    df[f\"{col}_month_sin\"] = np.sin(2*np.pi*(df[f\"{col}_month\"]-1)/12)\n",
        "    df[f\"{col}_month_cos\"] = np.cos(2*np.pi*(df[f\"{col}_month\"]-1)/12)\n",
        "    df[f\"{col}_hour_sin\"]  = np.sin(2*np.pi*df[f\"{col}_hour\"]/24)\n",
        "    df[f\"{col}_hour_cos\"]  = np.cos(2*np.pi*df[f\"{col}_hour\"]/24)\n",
        "\n",
        "def frequency_encode(train_col: pd.Series, test_col: pd.Series):\n",
        "    freq = train_col.value_counts(dropna=False)\n",
        "    train_fe = train_col.map(freq)\n",
        "    test_fe  = test_col.map(freq).fillna(0)\n",
        "    return train_fe, test_fe\n",
        "\n",
        "def cv_target_encode(X, y, X_test, col, n_splits=5, noise_std=0.0):\n",
        "    \"\"\"Leakage-safe KFold target encoding (mean encoding).\"\"\"\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    oof = pd.Series(index=X.index, dtype=float)\n",
        "    global_mean = y.mean()\n",
        "    for tr_idx, val_idx in kf.split(X):\n",
        "        m = X.iloc[tr_idx][col].to_frame().join(y.iloc[tr_idx])\n",
        "        means = m.groupby(col)[y.name].mean()\n",
        "        oof.iloc[val_idx] = X.iloc[val_idx][col].map(means).fillna(global_mean)\n",
        "    test_enc = X_test[col].map(X.join(y).groupby(col)[y.name].mean()).fillna(global_mean)\n",
        "    if noise_std > 0:\n",
        "        oof = oof + np.random.normal(0, noise_std, size=len(oof))\n",
        "    return oof.values, test_enc.values"
      ],
      "metadata": {
        "id": "LlRJwbndXjGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load"
      ],
      "metadata": {
        "id": "_4t2oA1UXxrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test  = pd.read_csv(TEST_PATH)\n",
        "id_col_test = find_id_col(test)\n",
        "\n",
        "assert TARGET_COL in train.columns, f\"'{TARGET_COL}' not found in Train.csv!\"\n",
        "\n",
        "# Try infer id column from sample submission, else from Test.csv\n",
        "if os.path.exists(SAMPLE_SUB_PATH):\n",
        "    sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "    id_col = sample_sub.columns[0]\n",
        "else:\n",
        "    id_col = find_id_col(train) if find_id_col(train) in train.columns else id_col_test\n",
        "\n",
        "# Keep IDs\n",
        "train_ids = train[id_col] if id_col in train.columns else pd.Series(range(len(train)))\n",
        "test_ids  = test[id_col_test]\n",
        "\n",
        "y = train[TARGET_COL].astype(float)"
      ],
      "metadata": {
        "id": "flMoS9TCVpAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify columns and parse datetimes"
      ],
      "metadata": {
        "id": "wNXx2lpUX6Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.api.types import is_datetime64_any_dtype"
      ],
      "metadata": {
        "id": "4g5c4YgNYp5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude target and any id-like columns\n",
        "drop_cols = set([TARGET_COL])\n",
        "for c in train.columns:\n",
        "    if c.lower() == \"id\" or re.search(r'\\bid\\b$', c, flags=re.IGNORECASE):\n",
        "        drop_cols.add(c)\n",
        "\n",
        "feature_cols = [c for c in train.columns if c not in drop_cols]\n",
        "\n",
        "# tentative datetime detection on object-like cols\n",
        "dt_candidates = []\n",
        "X_train_raw = train[feature_cols].copy()\n",
        "X_test_raw  = test[[c for c in feature_cols if c in test.columns]].copy()\n",
        "\n",
        "for c in feature_cols:\n",
        "    if X_train_raw[c].dtype == \"O\" or np.issubdtype(X_train_raw[c].dtype, np.integer):\n",
        "        parsed = try_parse_datetime(X_train_raw[c])\n",
        "        # if np.issubdtype(parsed.dtype, np.datetime64):\n",
        "        if is_datetime64_any_dtype(parsed):\n",
        "            # also parse test\n",
        "            parsed_test = pd.to_datetime(X_test_raw[c], errors=\"coerce\", utc=True)\n",
        "            X_train_raw[c] = parsed\n",
        "            X_test_raw[c]  = parsed_test\n",
        "            dt_candidates.append(c)"
      ],
      "metadata": {
        "id": "CiM2JnMoVZli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "ICG1YU4kYumD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Date/time features\n",
        "for c in dt_candidates:\n",
        "    add_datetime_features(X_train_raw, c)\n",
        "    add_datetime_features(X_test_raw, c)\n",
        "\n",
        "# Option: drop original datetime columns to avoid leakage via exact timestamps\n",
        "X_train_raw.drop(columns=dt_candidates, inplace=True)\n",
        "X_test_raw.drop(columns=[c for c in dt_candidates if c in X_test_raw.columns], inplace=True)\n",
        "\n",
        "# 2) Basic text/route lengths for object cols (non-datetime)\n",
        "obj_cols = [c for c in X_train_raw.columns if X_train_raw[c].dtype == \"O\"]\n",
        "for c in obj_cols:\n",
        "    X_train_raw[f\"{c}_len\"] = X_train_raw[c].astype(str).str.len()\n",
        "    X_test_raw[f\"{c}_len\"]  = X_test_raw[c].astype(str).str.len()\n",
        "\n",
        "# 3) Frequency encoding for medium/high-cardinality categoricals\n",
        "# Decide which to frequency-encode vs ordinal-encode\n",
        "cat_cols = [c for c in X_train_raw.columns if X_train_raw[c].dtype == \"O\"]\n",
        "med_hi_cat = []\n",
        "low_cat    = []\n",
        "for c in cat_cols:\n",
        "    nunique = X_train_raw[c].nunique(dropna=False)\n",
        "    if nunique > 20:\n",
        "        med_hi_cat.append(c)\n",
        "    else:\n",
        "        low_cat.append(c)\n",
        "\n",
        "for c in med_hi_cat:\n",
        "    tr_fe, te_fe = frequency_encode(X_train_raw[c].astype(\"category\"), X_test_raw[c].astype(\"category\"))\n",
        "    X_train_raw[f\"{c}_freq\"] = tr_fe\n",
        "    X_test_raw[f\"{c}_freq\"]  = te_fe\n",
        "\n",
        "# 4) Safe target encoding for very high-cardinality cols (optional, off by default)\n",
        "# If desired, uncomment to apply to (say) top two biggest categoricals.\n",
        "# high_card_sorted = sorted(med_hi_cat, key=lambda col: X_train_raw[col].nunique(dropna=False), reverse=True)[:2]\n",
        "# for c in high_card_sorted:\n",
        "#     tr_te, te_te = cv_target_encode(X_train_raw, y, X_test_raw, c, n_splits=N_SPLITS, noise_std=0.0)\n",
        "#     X_train_raw[f\"{c}_te\"] = tr_te\n",
        "#     X_test_raw[f\"{c}_te\"]  = te_te\n",
        "\n",
        "# After engineered features, remove raw object columns; we’ll ordinal-encode the small ones.\n",
        "X_train_proc = X_train_raw.copy()\n",
        "X_test_proc  = X_test_raw.copy()\n",
        "\n",
        "# Keep low-cardinality categoricals for ordinal encoding\n",
        "to_oe = low_cat\n",
        "\n",
        "# Fill obvious NA before encoding\n",
        "for c in X_train_proc.columns:\n",
        "    if c in to_oe:\n",
        "        X_train_proc[c] = X_train_proc[c].astype(\"object\").fillna(\"__NA__\")\n",
        "        X_test_proc[c]  = X_test_proc[c].astype(\"object\").fillna(\"__NA__\")\n",
        "    elif pd.api.types.is_numeric_dtype(X_train_proc[c]):\n",
        "        med = X_train_proc[c].median()\n",
        "        X_train_proc[c] = X_train_proc[c].fillna(med)\n",
        "        if c in X_test_proc.columns:\n",
        "            X_test_proc[c]  = X_test_proc[c].fillna(med)\n",
        "\n",
        "# Ordinal encode low-cardinality categoricals (consistent fit on combined)\n",
        "if to_oe:\n",
        "    oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, dtype=\"int64\")\n",
        "    comb = pd.concat([X_train_proc[to_oe], X_test_proc[to_oe]], axis=0)\n",
        "    comb_enc = oe.fit_transform(comb)\n",
        "    X_train_proc[to_oe] = comb_enc[:len(X_train_proc)]\n",
        "    X_test_proc[to_oe]  = comb_enc[len(X_train_proc):]\n",
        "\n",
        "# Drop raw medium/high-cardinality object cols (we kept *_freq and *_len)\n",
        "X_train_proc.drop(columns=med_hi_cat, inplace=True, errors=\"ignore\")\n",
        "X_test_proc.drop(columns=[c for c in med_hi_cat if c in X_test_proc.columns], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# Final safety alignment\n",
        "X_test_proc = X_test_proc.reindex(columns=X_train_proc.columns, fill_value=0)\n",
        "\n",
        "# Optional clipping of extreme targets (robust to outliers)\n",
        "# y = y.clip(lower=0, upper=np.percentile(y, 99.5))\n"
      ],
      "metadata": {
        "id": "dj4PI2vjX_0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CV Strategy (time-aware if possible)"
      ],
      "metadata": {
        "id": "1lfmbn4mY4Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If any engineered datetime features exist, prefer TimeSeriesSplit using the *earliest* parsed datetime column\n",
        "time_split = None\n",
        "if dt_candidates:\n",
        "    # Pick the earliest parsed DT column as ordering anchor\n",
        "    # Re-create a timestamp from features to sort (year, month, day, hour) — robust if some parts missing\n",
        "    anchor = dt_candidates[0]  # first parsed column\n",
        "    # We had dropped original, but we can reconstruct an ordering proxy:\n",
        "    # use the derived numeric features\n",
        "    base_cols = [f\"{anchor}_year\", f\"{anchor}_month\", f\"{anchor}_day\", f\"{anchor}_hour\"]\n",
        "    has_all = all(c in X_train_raw.columns for c in base_cols)\n",
        "    if has_all:\n",
        "        order_key = (X_train_raw[f\"{anchor}_year\"].fillna(1970).astype(int) * 10_000_000 +\n",
        "                     X_train_raw[f\"{anchor}_month\"].fillna(1).astype(int) * 100_000 +\n",
        "                     X_train_raw[f\"{anchor}_day\"].fillna(1).astype(int) * 1_000 +\n",
        "                     X_train_raw[f\"{anchor}_hour\"].fillna(0).astype(int))\n",
        "        order = np.argsort(order_key.values)\n",
        "        time_split = TimeSeriesSplit(n_splits=N_SPLITS)\n",
        "    else:\n",
        "        time_split = None"
      ],
      "metadata": {
        "id": "wbW7Np_pY16n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightGBM Model & CV"
      ],
      "metadata": {
        "id": "XwgZ4kxoY_Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade lightgbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "heAc0CJPZMKC",
        "outputId": "b08e3bf9-6287-4bc1-fb71-1013f2527e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm\n",
        "print(lightgbm.__version__)\n",
        "print(lightgbm.__file__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuavhU_zZjS2",
        "outputId": "546cc2db-9a13-4910-ae6f-ff779f4cb565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.6.0\n",
            "/usr/local/lib/python3.11/dist-packages/lightgbm/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ire3UBeJaDXj",
        "outputId": "d5c6bfba-432e-40ac-db38-44cc3c0db80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_params = dict(\n",
        "    n_estimators=5000,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    num_leaves=63,\n",
        "    max_depth=-1,\n",
        "    reg_alpha=0.0,\n",
        "    reg_lambda=2.0,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "oof = np.zeros(len(X_train_proc))\n",
        "test_pred = np.zeros(len(X_test_proc))\n",
        "rmses = []\n",
        "\n",
        "if time_split is not None:\n",
        "    splitter = time_split\n",
        "    idx_seq = np.arange(len(X_train_proc))\n",
        "else:\n",
        "    splitter = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    idx_seq = None\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(splitter.split(X_train_proc if idx_seq is None else idx_seq)):\n",
        "    if idx_seq is not None:\n",
        "        tr_idx, val_idx = tr_idx, val_idx  # already indices\n",
        "    X_tr, X_val = X_train_proc.iloc[tr_idx], X_train_proc.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
        "\n",
        "    model = LGBMRegressor(**lgb_params)\n",
        "    model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=\"rmse\",\n",
        "        # callbacks=[],\n",
        "        callbacks=[lightgbm.early_stopping(stopping_rounds=50)]\n",
        "        # early_stopping_rounds=200\n",
        "    )\n",
        "\n",
        "    val_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n",
        "    oof[val_idx] = val_pred\n",
        "    # rmse = mean_squared_error(y_val, val_pred, squared=False)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "    rmses.append(rmse)\n",
        "    print(f\"Fold {fold+1}/{N_SPLITS} RMSE: {rmse:.4f}\")\n",
        "\n",
        "    test_pred += model.predict(X_test_proc, num_iteration=model.best_iteration_) / N_SPLITS\n",
        "\n",
        "# cv_rmse = mean_squared_error(y, oof, squared=False)\n",
        "cv_rmse = np.sqrt(mean_squared_error(y, oof))\n",
        "print(f\"OOF RMSE: {cv_rmse:.4f} | Fold RMSEs: {np.round(rmses, 4)}\")\n",
        "\n",
        "# Non-negative delays (optional but reasonable)\n",
        "test_pred = np.clip(test_pred, a_min=0, a_max=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxCD-1EIY89i",
        "outputId": "c316e349-33af-475c-9a7f-58b42139b790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001264 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 603\n",
            "[LightGBM] [Info] Number of data points in the train set: 17973, number of used features: 21\n",
            "[LightGBM] [Info] Start training from score 39.043065\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[11]\tvalid_0's rmse: 91.8894\tvalid_0's l2: 8443.66\n",
            "Fold 1/5 RMSE: 91.8894\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006960 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 661\n",
            "[LightGBM] [Info] Number of data points in the train set: 35945, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 34.224426\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[82]\tvalid_0's rmse: 125.519\tvalid_0's l2: 15755\n",
            "Fold 2/5 RMSE: 125.5190\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004429 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 695\n",
            "[LightGBM] [Info] Number of data points in the train set: 53917, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 42.173248\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[378]\tvalid_0's rmse: 114.627\tvalid_0's l2: 13139.3\n",
            "Fold 3/5 RMSE: 114.6266\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089179 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 700\n",
            "[LightGBM] [Info] Number of data points in the train set: 71889, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 43.715561\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[144]\tvalid_0's rmse: 124.445\tvalid_0's l2: 15486.7\n",
            "Fold 4/5 RMSE: 124.4455\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019437 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 718\n",
            "[LightGBM] [Info] Number of data points in the train set: 89861, number of used features: 24\n",
            "[LightGBM] [Info] Start training from score 47.747888\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[523]\tvalid_0's rmse: 125.792\tvalid_0's l2: 15823.7\n",
            "Fold 5/5 RMSE: 125.7923\n",
            "OOF RMSE: 113.7776 | Fold RMSEs: [ 91.8894 125.519  114.6266 124.4455 125.7923]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission"
      ],
      "metadata": {
        "id": "A6Xh1-Dea_KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    \"id\": test_ids,        # Must mirror Test.csv IDs\n",
        "    \"target\": test_pred\n",
        "})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Saved submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K09WG8yZGLw",
        "outputId": "84c71363-3f59-4104-8cd8-640847bd9903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JcsBTUL9bEIX",
        "outputId": "2d60d977-f941-47df-c563-4e6220348195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5d30a8b8-8731-4a01-840c-a3b6c82f4ce7\", \"submission.csv\", 291123)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKapu0WxbQup"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}